# Demystifying Terms Data Science, Machine Learning, Artificial Intelligence
## Artificial Intelligence
-Artificial Intelligence is the broad field of creating systems that can perform tasks that normally require human intelligence. These tasks can include things like understanding natural language, recognizing images, making decisions, or playing games. AI encompasses multiple technologies, including Machine Learning.
An example of AI is a virtual assistant like Siri or Alexa.
## Artificial General Intelligence
-Artifficial General Intelligence aims to perform any intellectual task that a human can do. It has the ability to plan, solve complex problems, and adapt to new situations, with general learning capabilities across multiple domains.
-eg: self driving car
## History of Artificial Intelligence
1) 1940-1950-> Alan Turing -> proposed the concept of a machine that could simulate any human intelligence, which became known as the Turing Machine. Also introduced the Turing Test, a method to determine if a machine could exhibit intelligent behavior.
2) 1950->John McCarthy, Marvin Minsky, Nathaniel Rochester, and others coined the term "Artificial Intelligence".
3) 1956-> Birth of AI
4) 1960-1970-> ELIZA -a chatbot created by Joseph Weizenbaum and SHRDLU - a program that could understand simple commands in a block world.
5) 1980-> The backpropagation algorithm in neural networks to  allowing systems to learn from data and improve over time.
6) 1990-> IBM developed Deep Blue.
7) 2000-> Machine learning algorithms became more sophisticated, with the rise of support vector machines and decision trees.
Companies like Google, Facebook, and Amazon began using AI to improve services like search, recommendations, and advertisements.
8) 2010-> Development of Deep Learning.
9) 2020-> AI became increasingly integrated into daily life with applications in autonomous vehicles, healthcare, robotics, finance, entertainment, education and more.
Advances in transformer models, like GPT-3 and BERT, allowed for more advanced language understanding and generation.
### Perceptron
Perceptron is a type of linear classifier used for binary classification tasks. It will classify input data into one of two possible classes.
![image](https://github.com/user-attachments/assets/900994bf-d547-4a9a-a10e-bb5ac23e8131)


Structure of Perceptron:-
Input layer: The perceptron receives the input features like data points.
Weights: Each input has an associated weight that determines its importance.
Activation function: After the weighted sum of inputs is calculated, an activation function is used to produce the output.

Limitations:-
A single perceptron can only solve linearly separable problems i.e,problems where the two classes can be separated by a straight line or hyperplane, It cannot handle more complex patterns or non-linear separations.

### Backpropagation
The backpropagation algorithm in neural networks to  allowing systems to learn from data and improve over time.
It is a technique used to optimize the weights of a neural network by reducing the difference between the predicted output and the actual target output. The algorithm calculates the gradient of the loss function for each weight in the network and then modifies the weights to minimize the error.

### AI Winter
AI has experienced periods of optimism and setbacks known as "AI winters".
It refers to periods in the history of artificial intelligence research when progress slowed down significantly, and enthusiasm for AI waned due to unmet expectations, lack of funding, and failures in achieving the ambitious goals set by researchers.
i) In the early years of AI research, there was significant excitement about the potential of AI, but by the mid-1970s, progress began to slow down.
ii)After a brief period of optimism in the 1980s, particularly around expert systems, the limitations of AI became apparent. Many of the systems were expensive and inflexible.

Causes of AI Winter:-
Overhyped Expectations, Technological Limitations, Lack of Generalisation.

Recovery:-
AI winters were followed by periods of renewal and growth as new techniques, like machine learning, deep learning, and better hardware (e.g., GPUs), helped overcome previous limitations. 
### DARPA Grand Challenge 2005
It was a landmark event in autonomous vehicle development, organized by the Defense Advanced Research Projects Agency (DARPA).
It was designed to push the boundaries of autonomous driving technologies, challenging teams to build self-driving vehicles capable of navigating a complex off-road course without human intervention.

### ImageNet 2012
ImageNet is a publicly-available large-scale database with annotated images, composed to be used in multiple computer vision tasks.
It refers to the dataset used in the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark competition for image classification where algorithms are tasked with identifying objects within images across a wide range of categories.

Keypoints:-
Large scale: It contains a vast number of images across various categories, enabling training of robust image recognition models. 
WordNet hierarchy: Images are organized based on the WordNet system, allowing for semantic relationships between different object classes. 
AlexNet breakthrough: The 2012 ImageNet challenge saw a significant performance leap with the introduction of AlexNet, a convolutional neural network architecture, which significantly boosted interest in deep learning. 

### 2024 Nobel Prize in Physics
John J. Hopfield and Geoffrey E. Hinton for their foundational discoveries and inventions that enable machine learning with artificial neural networks.

### Deepseek
DeepSeek is a Chinese artificial intelligence (AI) company.It's known for its open-source AI models that compete with those from OpenAI, but at a lower cost. 
DeepSeek models can generate human-like responses, help with coding, and solve problems.
DeepSeek models are available through a web interface, mobile app, and API access. 
DeepSeek publishes its methods and makes its models available to researchers.

## Key Deciding Factors of Growth of AI
1. Huge amount of data.
2. Computing power
3. Algorithmic Improvements.

## Data Science, Machine Learning and Artificial Intelligence
-Data Science is the process of collecting, analyzing, and interpreting vast amounts of data to help inform decisions. It combines a variety of fields, like statistics, computer science, and domain expertise, to extract meaningful insights from data.

-Machine Learning is a subset of Artificial Intelligence (AI) that focuses on algorithms that allow computers to learn from and make predictions or decisions based on data. A machine learning model enhances its performance by learning from data, Rather than being directly programmed to carry out particular tasks.

-Artificial Intelligence is the broad field of creating systems that can perform tasks that normally require human intelligence. These tasks can include things like understanding natural language, recognizing images, making decisions, or playing games. AI encompasses multiple technologies, including Machine Learning.

## Data Science Application Domains
1. Finance
2. Health care
3. Transportation
4. Retail and E-commerce
5. Sports Analytics
6. Education
7. Govt and public sectors
8. Telecommunications
9. Cybersecurity

## Correlation Vs. Causation
Correlation:-
AI can readily identify patterns and relationships between data points, showing a correlation between variables, but this doesn't necessarily mean one variable is causing the other. 
Causation:-
AI needs more advanced techniques to analyze the "why" behind the relationship, allowing it to determine which variable is actually influencing the other. 

## Ethics in AI
1. Bias and Fairness
2. Privacy
3. Accountability
4. Job displacement
5. Safety and security
